{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab2: In this lab we will calculate the bias and variance for different prediction models and determine the best model for a synthetic dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usefull imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression \n",
    "from sklearn.preprocessing import PolynomialFeatures \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np \n",
    "import random \n",
    "import pickle \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some global variables that can be used to check specific outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = 1  # 1 if you want to see the variable values during the program execution\n",
    "graphing = 1  # 1 to see the graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: LOADING THE DATASET AND VISUALISING IT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load the data_set from data.pkl file. The dataset contains pairs of points <x,y>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: read dataset (data.pkl) using pandas.read_pickle. Note: this command will return a numpy array \n",
    "data_set=  \n",
    "#TODO: print the size of the dataset i.e the length of the numpy array\n",
    "data_set_size =  \n",
    "print(data_set_size )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(debug==1):\n",
    "    #TODO: print the first 5 elements of the dataset \n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split The Dataset Into X and Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we split the <x,y> pairs into x and y separate arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: store x and y in seperate arrays\n",
    "x = \n",
    "y = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(debug==1):\n",
    "    #TODO: print the first 5 elements of x and y\n",
    "    print(\"X: \",)\n",
    "    print(\"Y: \",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphing the given dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we plot the given dataset, just to get the feel of the dataset provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: plot the dataset\n",
    "fig = plt.figure()\n",
    "plt.plot(, , 'r.', markersize=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: RESAMPLING DATA SETS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split The Dataset Into Testing And Training Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This line splits the dataset into the testing and training datasets in the ratio 1:9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: split the dataset into training set and testing set with ratio 9:1 using tran_test_split function while shuffling the dataset Note: set random_state = 3\n",
    "xTrain, xTest, yTrain, yTest = \n",
    "#TODO: save the size of the training and testing sets \n",
    "test_data_size = \n",
    "train_data_size = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(debug==1):\n",
    "    #TODO: print the size of the training and testing sets\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split The Training Dataset Into 10 Different Training Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we run a loop 10 times and store the train dataset values from start_in to the end_in as a numpy array in a list (X_train_data_sets and Y_train_data_sets), and update the values of start_in and end_in in each iteration of the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_data_sets = list()\n",
    "Y_train_data_sets = list()\n",
    "start_in = 0\n",
    "train_data_sets_size = int(train_data_size/10)\n",
    "end_in = train_data_sets_size\n",
    "for i in range(10):\n",
    "    #TODO: Append xTrain data from start index to end index to X_train_data_sets\n",
    "    X_train_data_sets.append()\n",
    "    #TODO: Append yTrain data from start index to end index to y_train_data_sets\n",
    "    Y_train_data_sets.append()\n",
    "    #TODO: Update the start index and the end index to start from where the last dataset finished\n",
    "    start_in += \n",
    "    end_in += "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(debug==1):\n",
    "    #TODO: Print the size of the splitted datasets Hint: all of them have the same size \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphing Each Of The Training Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we graph each of the training datasets separately. (to check if the datasets are sampled properly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(debug==1):\n",
    "    for i in range(10):\n",
    "        print(\"Training set \",i)\n",
    "        fig = plt.figure()\n",
    "        #TODO plot the current dataset, use red dots and markersize = 2\n",
    "        plt.plot()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3: TRAINING A MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting A Graph Of The Trained Polynomial Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we take each of the training datasets, and plot the training dataset points and the values predicted by the model on the test dataset points, to visualise the provided data.\n",
    "For each training set, 9 graphs are plotted, each corresponding to the model of each degree (from 0 to 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = []\n",
    "for i in range(10):\n",
    "    print(\"TRAINING SET \", i)\n",
    "    f = plt.figure()\n",
    "    f, axes = plt.subplots(nrows=3, ncols=3, sharex=True, sharey=True, figsize=(30, 30))\n",
    "    x = X_train_data_sets[i][:, np.newaxis]  # transposing it\n",
    "    y = Y_train_data_sets[i]\n",
    "    temp = []\n",
    "    for degree in range(0, 9):\n",
    "        axes[int(degree/3)][int(degree % 3)].plot(x, y, 'r.', markersize=4)\n",
    "        #TODO: Create polynomial features of current degree + 1 using PolynomialFeatures function\n",
    "        poly_features = PolynomialFeatures(degree=)\n",
    "        #TODO: Fit the polynomial features on the current training set using the fit_transform method\n",
    "        X_train_poly = poly_features.fit_transform()\n",
    "        #TODO: Create a plolynomial regression model using LinearRegression class\n",
    "        poly_model = \n",
    "        #TODO: Fit the model to the current training set\n",
    "        poly_model.fit(, )\n",
    "        #TODO: Predict the value of y for the testing set using the predict method Hint: fit the polynomial features on the xTest[:, numpy.newaxis] then predict the value of y for the polynomial features\n",
    "        y_test_predict = poly_model.predict()\n",
    "        #TODO: append the current prediction to the temp list\n",
    "        temp.append()\n",
    "        axes[int(degree/3)][int(degree % 3)].plot(xTest[:, np.newaxis], y_test_predict, 'b.', markersize=4)\n",
    "        plt.title(\"DEGREE \"+str(degree+1))\n",
    "        plt.xlabel(\"X\")\n",
    "        plt.ylabel(\"Y\")\n",
    "    #TODO: append the prediction of all 9 models to the y_predicted list Hint: the prediction of all models are saved in temp\n",
    "    y_predicted.append()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 4: CALCULATE THE BIAS AND VARIANCE OF THE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for calculating the bias and the variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " we calculate the bias and variance as follows:\n",
    "- For a given degree we append the values of the y_predicted for each dataset to a list\n",
    "- Convert this list to a numpy array y_predicted_part\n",
    "- Calculate the bias of this list by subtracting the prediction of the model for each training set from the testing dataset\n",
    "- Bias corresponding to the models of a given degree is the mean of this list\n",
    "- Similarly calculate the variance of this list\n",
    "- Variance corresponding to the models of a given degree is the mean of this list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bias_variance(order):\n",
    "    y_predicted_part = []\n",
    "    for i in range(10):\n",
    "        y_predicted_part.append(y_predicted[i][order])\n",
    "    y_predicted_part = np.asarray(y_predicted_part)\n",
    "    #TODO: calculate the bias using np.mean and np.abs\n",
    "    bias=np.mean(,axis=) \n",
    "    #TODO: calculate the variance using np.var\n",
    "    variance=np.var(,axis=) \n",
    "    return(np.mean(bias), np.mean(variance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we call the function as follows, in order to populate the lists, bias and variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias = []\n",
    "variance = []\n",
    "for i in range(9):\n",
    "    b, v = find_bias_variance(i)\n",
    "    bias.append(b)\n",
    "    variance.append(v)\n",
    "print(\"Bias:\", bias)\n",
    "print(\"Variance:\", variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lists, bias and variance, now contain the bias and variance corresponding to a particular degree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabulate the values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the pandas library in order to display the required items in a table format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_table = dict()\n",
    "final_table[\"DEGREE\"] = range(1, 10)\n",
    "final_table[\"BIAS\"] = bias\n",
    "final_table[\"BIAS^2\"] = list(np.array(bias)**2)\n",
    "final_table[\"VARIANCE\"] = variance\n",
    "final_table[\"MSE\"] = list(np.array(final_table[\"BIAS^2\"])+np.array(variance))\n",
    "df = pd.DataFrame(final_table)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the bias-variance tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(final_table[\"DEGREE\"], final_table[\"VARIANCE\"], color=\"blue\")\n",
    "plt.plot(final_table[\"DEGREE\"], final_table[\"BIAS^2\"], color=\"red\")\n",
    "plt.plot(final_table[\"DEGREE\"], final_table[\"MSE\"], color=\"green\")\n",
    "plt.title(\"BIAS VARIANCE TRADEOFF\")\n",
    "plt.xlabel(\"Degree\")\n",
    "plt.ylabel(\"Bias^2/Variance\")\n",
    "plt.legend([\"VARIANCE\", \"BIAS^2\", \"MSE\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FITTING THE TRAINED MODEL TO THE TESTING DATASET FOR DISPLAYING THE LINE OF BEST FIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure()\n",
    "f, axes = plt.subplots(nrows = 3, ncols = 3, sharex=True, sharey = True,figsize=(30,30))\n",
    "for degree in range(0,9):\n",
    "    xtemp=np.concatenate([xTest for i in range(10)])\n",
    "    y_predicted_part=[]\n",
    "    for i in range(10):\n",
    "        #TODO Append the prediction of the model with the current degree for each dataset to y_predicted_part\n",
    "        y_predicted_part.append()\n",
    "    ytemp=np.array(y_predicted_part).reshape(-1)\n",
    "    axes[int((degree)/3)][int((degree)%3)].plot(xTest, yTest, 'r.',markersize=10)\n",
    "    axes[int((degree)/3)][int((degree)%3)].plot(xtemp, ytemp,'b.',markersize=1)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK**:  \n",
    "- You have been provided with a training data and a testing data. You need to fit the given data to polynomials of degree 1 to 9(both inclusive). \n",
    "- Specifically, you have been given 20 subsets of training data containing 400 samples each. For each polynomial, create 20 models trained on the 20 different subsets and find the variance of the predictions on the testing data. Also, find the bias of your trained models on the testing data. Finally plot the bias-variance trade-Off graph. \n",
    "- Write your observations in the report with respect to underfitting, overfitting and also comment on the type of data just by looking at the bias-variance plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usefull imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy\n",
    "import random\n",
    "import pickle\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 1: LOADING THE DATASET AND VISUALISING IT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load the data_set from multiple files stored in the same directory as the current notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get x train dataset\n",
    "f = open('X_train.pkl', 'rb')\n",
    "X_train_data_sets = pickle.load(f)\n",
    "f.close()\n",
    "# Get y train dataset\n",
    "f = open('Y_train.pkl', 'rb')\n",
    "Y_train_data_sets = pickle.load(f)\n",
    "f.close()\n",
    "# Get x test dataset\n",
    "f = open('X_test.pkl', 'rb')\n",
    "xTest = pickle.load(f)\n",
    "f.close()\n",
    "# Get y test dataset\n",
    "f = open('Fx_test.pkl', 'rb')\n",
    "yTest = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(debug == 1): #TODO: Print traning dataset and testing dataset and the length of the training dataset\n",
    "    print('Training data: ')\n",
    "    print('X_train dataset: ', X_train_data_sets)\n",
    "    print('Y_train dataset: ', Y_train_data_sets) \n",
    "    print('--------------------------------------------------------------')\n",
    "    print('Testing data: ') \n",
    "    print('X test: ',xTest) \n",
    "    print('y test: ',yTest)\n",
    "    print (len(X_train_data_sets))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (0,20): \n",
    "    plt.figure()\n",
    "    plt.plot(X_train_data_sets[i],Y_train_data_sets[i],\"r.\",markersize=2) \n",
    "    plt.grid()\n",
    "    plt.xlabel('XTrain') \n",
    "    plt.ylabel('YTrain')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing data \n",
    "plt.plot(xTest,yTest,\"b.\",markersize=2) \n",
    "plt.grid() \n",
    "plt.xlabel('XTest') \n",
    "plt.ylabel('YTest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting A Graph Of The Trained Polynomial Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = []\n",
    "for i in range(20):\n",
    "    print(\"TRAINING SET \", i)\n",
    "    f = plt.figure()\n",
    "    f, axes = plt.subplots(nrows=3, ncols=3, sharex=True, sharey=True, figsize=(30, 30))\n",
    "    x = X_train_data_sets[i][:, np.newaxis]  # transposing it\n",
    "    y = Y_train_data_sets[i]\n",
    "    temp = []\n",
    "    for degree in range(0, 9):\n",
    "        axes[int(degree/3)][int(degree % 3)].plot(x, y, 'r.', markersize=4)\n",
    "        #TODO: Create polynomial features of current degree + 1 using PolynomialFeatures function\n",
    "        \n",
    "        #TODO: Fit the polynomial features on the current training set using the fit_transform method\n",
    "        \n",
    "        #TODO: Create a plolynomial regression model using LinearRegression class\n",
    "        \n",
    "        #TODO: Fit the model to the current training set\n",
    "        \n",
    "        #TODO: Predict the value of y for the testing set using the predict method Hint: fit the polynomial features on the xTest[:, numpy.newaxis] then predict the value of y for the polynomial features\n",
    "        \n",
    "        #TODO: append the current prediction to the temp list\n",
    "        \n",
    "        axes[int(degree/3)][int(degree % 3)].plot(xTest[:, np.newaxis], y_test_predict, 'b.', markersize=4)\n",
    "        plt.title(\"DEGREE \"+str(degree+1))\n",
    "        plt.xlabel(\"X\")\n",
    "        plt.ylabel(\"Y\")\n",
    "    #TODO: append the prediction of all 9 models to the y_predicted list Hint: the prediction of all models are saved in temp\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 3: CALCULATE THE BIAS AND VARIANCE OF THE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for calculating the bias and the variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bias_variance1(order): \n",
    "    #TODO: define a function to calculate mean and variance for 20 datasets\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: call the function to calculate the bias and variance for all the degrees of the models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabulate the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Tabulate the values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the bias-variance tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Plot the bias^2, variance, and MSE \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FITTING THE TRAINED MODEL TO THE TESTING DATASET FOR DISPLAYING THE LINE OF BEST FIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Plot the fitting function of all models for the 20 datasets\n"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
